# 운동 동작 분류 AI 경진대회
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import warnings
from google.colab import drive
drive.mount('/content/drive')

warnings.filterwarnings(action='ignore')
%matplotlib inline
# 데이터 불러오기
cd drive
ls
train_features = pd.read_csv('MyDrive/train_features.csv')
train_labels = pd.read_csv('MyDrive/train_labels.csv')
test_features = pd.read_csv('MyDrive/test_features.csv')
submission = pd.read_csv('MyDrive/sample_submission.csv')


# Feature Engineering

4초씩 나눠서 3등분 한다,

0.02초씩 측정된 운동에 따른 acc, gy의 x, y, z 축에 센서에 따른 측정 값이다. 

- Max : 최댓값 

- Min : 최소값 

- Mean : 평균값 

- Q1 : 1사분위수 

- Q2 : 중앙값 

- Q3 : 3사분위수 

- IQR : Q3 - Q1 

- MAD : Mean Absolute Deviation 
    - 평균 절대 편차(Mean Absolute Deviation)
    - 평균과 개별 관측치 사이 거리의 평균이다. 각 측정치에서 전체 평균 값을 뺀 값의 절댓값으로 표시되는 편차들의 합에서 산술평균을 말한다.

- RMS : Root Mean Squre 
    - 제곱평균제곱근 (Root mean square)
    - 수학에서, 제곱평균제곱근 혹은 이차평균은 변화하는 값의 크기에 대한 통계적 척도이다. 이것은 특히 사인함수처럼 변수들이 음과 양을 오고 갈 때에 유용하다

- ZCR : Zero Crossing Rate 
    - 제로 크로싱 속도는 신호가 양수에서 0으로, 또는 음에서 0에서 양으로 변경되는 속도입니다.


참고: https://www.researchgate.net/publication/311591075_Feature_Analysis_to_Human_Activity_Recognition


#### 컬럼 생성

### 기존에 train_features 에 있던 컬럼으로 cols 생성
cols = ['acc_x', 'acc_y', 'acc_z', 'gy_x', 'gy_y', 'gy_z']
### 위의 feature engineering 원하는 값으로 vals 생성
vals = ['max', 'min', 'mean', 'Q1', "Q2", "Q3", "IQR", "mad", "rms", "zcr"]
### cols와 vals를 조합한 뒤에, 1부터 3까지의 값을 덧붙여주기 위한 seconds 변수 생성
seconds = [str(i) for i in range(1, 4)]
### cols, vals, seconds 세개를 합쳐주기 위한 res_cos 생성 
res_cols = []

for col in cols:
    for val in vals:
        for second in seconds:
            res_cols.append(col + '_' + val + '_' + second)
            
train_result = []
test_result = []
for col in cols:
  ##### train 용 10가지 feature(max, min, mean, Q1, Q2, Q3, IQR, mad, rms, zcr) 만들어주기

  # max
  train_max = train_features[[col]].values.reshape(-1,200).max(axis=1)
  train_max = train_max.reshape(-1,3)

  # min
  train_min = train_features[[col]].values.reshape(-1,200).min(axis=1)
  train_min = train_min.reshape(-1,3)

  # mean
  train_mean = train_features[[col]].values.reshape(-1,200).mean(axis=1)
  train_mean = train_mean.reshape(-1,3)

  # Q1, Q2, Q3
  train_Q1 = []
  train_Q2 = []
  train_Q3 = []
  for i in (train_features[[col]].values.reshape(-1,200)):
    train_Q1.append(np.percentile(i, 25))
    train_Q2.append(np.percentile(i, 50))
    train_Q3.append(np.percentile(i, 75))

  train_Q1 = np.array(train_Q1).reshape(-1,3)
  train_Q2 = np.array(train_Q2).reshape(-1,3)
  train_Q3 = np.array(train_Q3).reshape(-1,3)

  # IQR
  train_IQR = train_Q3 - train_Q1

  # mad
  train_mad = (abs(train_features[[col]].values.reshape(-1,200) - train_features[[col]].values.reshape(-1,200).mean(axis=1).reshape(-1,1))).mean(axis=1).reshape(-1,3)

  # rms
  train_rms = np.square((train_features[[col]].values.reshape(-1,200)**2).mean(axis=1)).reshape(-1,3)

  # zcr
  train_zcr = ((train_features[[col]].values.reshape(-1,200)[:,:-1] * train_features[[col]].values.reshape(-1,200)[:,1:] < 0).sum(axis=1) / 200 ).reshape(-1,3)


  # 합쳐주기 
  train_val = np.concatenate([train_max, train_min, train_mean, train_Q1, train_Q2, train_Q3, train_IQR, train_mad, train_rms, train_zcr],axis=1)
  train_result.append(train_val)

  ##### test 용 10가지 feature(max, min, mean, Q1, Q2, Q3, IQR, mad, rms, zcr) 만들어주기

  # max
  test_max = test_features[[col]].values.reshape(-1,200).max(axis=1)
  test_max = test_max.reshape(-1,3)

  # min
  test_min = test_features[[col]].values.reshape(-1,200).min(axis=1)
  test_min = test_min.reshape(-1,3)

  #mean
  test_mean = test_features[[col]].values.reshape(-1,200).mean(axis=1)
  test_mean = test_mean.reshape(-1,3)

  # Q1, Q2, Q3
  test_Q1 = []
  test_Q2 = []
  test_Q3 = []
  for i in (test_features[[col]].values.reshape(-1,200)):
    test_Q1.append(np.percentile(i, 25))
    test_Q2.append(np.percentile(i, 50))
    test_Q3.append(np.percentile(i, 75))

  test_Q1 = np.array(test_Q1).reshape(-1,3)
  test_Q2 = np.array(test_Q2).reshape(-1,3)
  test_Q3 = np.array(test_Q3).reshape(-1,3)

  # IQR
  test_IQR = test_Q3 - test_Q1

  # mad
  test_mad = (abs(test_features[[col]].values.reshape(-1,200) - test_features[[col]].values.reshape(-1,200).mean(axis=1).reshape(-1,1))).mean(axis=1).reshape(-1,3)

  # rms
  test_rms = np.square((test_features[[col]].values.reshape(-1,200)**2).mean(axis=1)).reshape(-1,3)

  # zcr
  test_zcr = ((test_features[[col]].values.reshape(-1,200)[:,:-1] * test_features[[col]].values.reshape(-1,200)[:,1:] < 0).sum(axis=1) / 200 ).reshape(-1,3)

  # 합쳐주기
  test_val = np.concatenate([test_max, test_min, test_mean, test_Q1, test_Q2, test_Q3, test_IQR, test_mad, test_rms, test_zcr],axis=1)
  test_result.append(test_val)


train_result = np.concatenate(train_result,axis=1)
test_result = np.concatenate(test_result,axis=1)

train_features_3 = pd.DataFrame(train_result, columns=res_cols)
test_features_3 = pd.DataFrame(test_result, columns=res_cols)
# reshape 해준 10가지 피쳐들의 shape 확인
[each.shape for each in [train_max, train_min, train_mean, train_Q1, train_Q2, train_Q3, train_IQR, train_mad, train_rms, train_zcr]]
#바꾼 변수들이 어떻게 생겼나 확인해보기
pd.DataFrame(train_val)
#바꾼 변수들이 어떻게 생겼나 확인해보기
pd.DataFrame(train_result)
train_val = train_features.drop('time', axis=1).groupby('id').agg(['mean', 'min', 'max', 'median']).values
test_val = test_features.drop('time', axis=1).groupby('id').agg(['mean', 'min', 'max', 'median']).values

ag_cols = ['acc_x',	'acc_y',	'acc_z',	'gy_x',	'gy_y',	'gy_z']
des_cols = ['mean', 'min', 'max', 'median']
result_cols = []

for ag_col in ag_cols:
  for des_col in des_cols:
    result_cols.append(ag_col+'_'+des_col)

train_mean_min_max_median_feature = pd.DataFrame(data = train_val, columns = result_cols)
test_mean_min_max_median_feature = pd.DataFrame(data = test_val, columns = result_cols)

train_mean_min_max_median_label = train_labels[['label']]
train_mean_min_max_median_label_dummy = pd.get_dummies(train_labels['label'])
train_features_3.head()
# 각각의 모델 평가 (튜닝 전)
# 데이터 분할
from sklearn.model_selection import StratifiedKFold

# 모델
from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgbm
import xgboost as xgb
from sklearn.linear_model import LogisticRegression

# 속도 체크
from tqdm import tqdm_notebook

n_split = 5
skf = StratifiedKFold(n_splits = n_split, shuffle=True, random_state=22)

rf_oof_train = np.zeros(train_mean_min_max_median_label_dummy.shape)
lgbm_oof_train = np.zeros(train_mean_min_max_median_label_dummy.shape)
xgb_oof_train = np.zeros(train_mean_min_max_median_label_dummy.shape)
lg_oof_train = np.zeros(train_mean_min_max_median_label_dummy.shape)

rf = RandomForestClassifier(random_state=22)
lgbm = lgbm.LGBMClassifier(random_state=22, 
                           tree_method='gpu_hist', 
                           predictor='gpu_predictor')
xgb = xgb.XGBClassifier(random_state=22, 
                        tree_method='gpu_hist', 
                        predictor='gpu_predictor')
lg = LogisticRegression(random_state=22)

for trn_idx, val_idx in tqdm_notebook(skf.split(train_features_3, train_mean_min_max_median_label)):
  trn_data, trn_label = train_features_3.iloc[trn_idx], train_mean_min_max_median_label.iloc[trn_idx]
  val_data, val_label = train_features_3.iloc[val_idx], train_mean_min_max_median_label.iloc[val_idx]

  rf.fit(trn_data, trn_label)
  rf_valid_pred = rf.predict_proba(val_data)
  rf_oof_train[val_idx] = rf_valid_pred

  lgbm.fit(trn_data, trn_label)
  lgbm_valid_pred = lgbm.predict_proba(val_data)
  lgbm_oof_train[val_idx] = lgbm_valid_pred

  xgb.fit(trn_data, trn_label)
  xgb_valid_pred = xgb.predict_proba(val_data)
  xgb_oof_train[val_idx] = xgb_valid_pred

  lg.fit(trn_data, trn_label)
  lg_valid_pred = lg.predict_proba(val_data)
  lg_oof_train[val_idx] = lg_valid_pred
# 평가
from sklearn.metrics import log_loss

print("lightgbm                      :{} ".format(log_loss(train_mean_min_max_median_label_dummy, lgbm_oof_train)))
print("RandomForestClassifier        :{} ".format(log_loss(train_mean_min_max_median_label_dummy, rf_oof_train)))
print("xgboost                       :{} ".format(log_loss(train_mean_min_max_median_label_dummy, xgb_oof_train)))
print("LogisticRegression            :{} ".format(log_loss(train_mean_min_max_median_label_dummy, lg_oof_train)))
# XGBClassifier (튜닝 후)

가장 성능이 좋았던 XGBClassifier를 하이퍼 파라미터 튜닝하여 사용했다.
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import log_loss
from tqdm import tqdm_notebook

n_split = 5
skf = StratifiedKFold(n_splits = n_split, shuffle=True, random_state=22)

# xgboost
xgb_oof_train = np.zeros(train_mean_min_max_median_label_dummy.shape)
xgb_oof_test = np.zeros(submission.drop('id', axis=1).shape)

model = xgb.XGBClassifier(random_state = 22, 
                          tree_method='gpu_hist', 
                          predictor='gpu_predictor',
                          learning_rate = 0.1,
                          n_estimators = 300,
                          max_depth = 6,
                          min_child_weight = 3,
                          gamma = 0.4,
                          subsample = 0.7,
                          colsample_bytree = 0.6)

for trn_idx, val_idx in tqdm_notebook(skf.split(train_features_3, train_mean_min_max_median_label)):
  trn_data, trn_label = train_features_3.iloc[trn_idx], train_mean_min_max_median_label.iloc[trn_idx]
  val_data, val_label = train_features_3.iloc[val_idx], train_mean_min_max_median_label.iloc[val_idx]

  model.fit(trn_data, trn_label)
  valid_pred = model.predict_proba(val_data)
  xgb_oof_train[val_idx] = valid_pred

  test_pred = model.predict_proba(test_features_3)
  xgb_oof_test += test_pred / n_split
from sklearn.metrics import log_loss

score = log_loss(train_mean_min_max_median_label_dummy, xgb_oof_train)
print('점수는: ', score)
from xgboost import plot_importance

fig, ax = plt.subplots(1,1,figsize=(15,50))
plot_importance(model, ax = ax)
# 최종 제출

oof 방식의 XGBClassifier 보다 전체 데이터를 통하여 학습한 XGBClassifier의 결과가 더 좋아서 전체 데이터를 기반으로 학습한 모델을 최종 모델로 결정했다.
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import log_loss
from tqdm import tqdm_notebook

model = xgb.XGBClassifier(random_state = 22, 
                          tree_method='gpu_hist', 
                          predictor='gpu_predictor',
                          learning_rate = 0.1,
                          n_estimators = 300,
                          max_depth = 6,
                          min_child_weight = 3,
                          gamma = 0.4,
                          subsample = 0.7,
                          colsample_bytree = 0.6)

model.fit(train_features_3, train_mean_min_max_median_label)
test_proba = model.predict_proba(test_features_3)
submission.iloc[:, 1:] = test_proba
submission.head()
결과 : 0.66207
